---
title: "Lab4"
author: "Bushong Boys"
date: "3/30/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(kernlab)
library(boot)
library(caret)

#Load data
data("spam")
tibble:: as.tibble(spam)


# Create test and training data 
set.seed(42)
# spam_idx = sample(nrow(spam), round(nrow(spam) / 2))
spam_idx = sample(nrow(spam), 1000)
spam_trn = spam[spam_idx, ]
spam_tst = spam[-spam_idx, ]

```

## Exercise 1

Using the spam data from the kernlab library, we looked to created classifier using a logistically to determine if an email was spam or not. The following four regression models were compared.

```{r eval = FALSE}
fit_caps = glm(type ~ capitalTotal,
               data = spam_trn, family = binomial)
fit_selected = glm(type ~ edu + money + capitalTotal + charDollar,
                   data = spam_trn, family = binomial)
fit_additive = glm(type ~ .,
                   data = spam_trn, family = binomial)
fit_over = glm(type ~ capitalTotal * (.),
               data = spam_trn, family = binomial, maxit = 50)

```
```{r echo = FALSE}
fit_caps = glm(type ~ capitalTotal,
               data = spam_trn, family = binomial)
fit_selected = glm(type ~ edu + money + capitalTotal + charDollar,
                   data = spam_trn, family = binomial)
fit_additive = glm(type ~ .,
                   data = spam_trn, family = binomial)
fit_over = glm(type ~ capitalTotal * (.),
               data = spam_trn, family = binomial, maxit = 50)

```

After fitting the data, we wanted to get a glimps into how the models performed. We used the following code, but noticed a similar issue as with ordinary linear regression that the misclassification rate goes down as you add more predictors despite the fact that model may be overfitting the to the training data. The results are below. 

```{r }
mean(ifelse(predict(fit_caps) > 0, "spam", "nonspam") != spam_trn$type)
mean(ifelse(predict(fit_selected) > 0, "spam", "nonspam") != spam_trn$type)
mean(ifelse(predict(fit_additive) > 0, "spam", "nonspam") != spam_trn$type)
mean(ifelse(predict(fit_over) > 0, "spam", "nonspam") != spam_trn$type)

```

To combat this problem, we decided to use a cross validation method using the cv.glm() function. We orginally ran a 5 fold validation with the seed set to one, then switched it to 100 fold with seed 90. I will not run the code due to the large number of messages it dispalys but below is the code run and summary of its output 

```{r eval= FALSE}
# First Case 
set.seed(1)
cv.glm(spam_trn, fit_caps, K = 5)$delta[1]
cv.glm(spam_trn, fit_selected, K = 5)$delta[1]
cv.glm(spam_trn, fit_additive, K = 5)$delta[1]
cv.glm(spam_trn, fit_over, K = 5)$delta[1]

#Second Case 
set.seed(90)
cv.glm(spam_trn, fit_caps, K = 100)$delta[1]
cv.glm(spam_trn, fit_selected, K = 100)$delta[1]
cv.glm(spam_trn, fit_additive, K = 100)$delta[1]
cv.glm(spam_trn, fit_over, K = 100)$delta[1]

```

First Case 
.216
.159
.087
.14

Second Case 
.216
.158
.081
.14

Models fit from most underfit to overfit are: caps, selected, additive, over
Models from best to worst are: additive, over, selected, caps
This does not change when the seed or K-folds are altered. Now that we explored cross validation, its time to use confusion matrices on our training data to further explore the success of our models and evaluate the best one to use in this case.

```{r}
# confusion matrix 
make_conf_mat = function(predicted, actual) {
  table(predicted = predicted, actual = actual)
}

# Give us predicted values (same output, different ways) 
caps_tst_pred = ifelse(predict(fit_caps, spam_tst) > 0,
                           "spam",
                           "nonspam")

selected_tst_pred = ifelse(predict(fit_selected, spam_tst) > 0,
                           "spam",
                           "nonspam")

additive_tst_pred = ifelse(predict(fit_additive, spam_tst) > 0,
                       "spam",
                       "nonspam")

over_tst_pred = ifelse(predict(fit_over, spam_tst) > 0,
                           "spam",
                           "nonspam")

# Create confusion matrices for each
caps_matrix = make_conf_mat(predicted = caps_tst_pred, actual = spam_tst$type)
caps_matrix
mean(caps_tst_pred != spam_tst$type)
sensitivity(caps_matrix)
specificity(caps_matrix)

selected_matrix = make_conf_mat(predicted = selected_tst_pred, actual = spam_tst$type)
selected_matrix
mean(selected_tst_pred != spam_tst$type)
sensitivity(selected_matrix)
specificity(selected_matrix)

additive_matrix = make_conf_mat(predicted = additive_tst_pred, actual = spam_tst$type)
additive_matrix
mean(additive_tst_pred != spam_tst$type)
sensitivity(additive_matrix)
specificity(additive_matrix)

over_matrix = make_conf_mat(predicted = over_tst_pred, actual = spam_tst$type)
over_matrix
mean(over_tst_pred != spam_tst$type)
sensitivity(over_matrix)
specificity(over_matrix)

```

In making the decision on what is the best model to use, we should first evaluate the overall accuracy of each model using their misclassification rate. Since borht the caps and selected models have relatively high rates (.34 and  .20 respectively), we can eliminate them from discussion. Instead, let's narrow ourselves down to the additive and over models with misclassification rates of .078 abd .156. It may be tempting to pick the additive model from this measure; however there is one additional factor we should still consider. 

In this scenario, it is a much costly error to have actual spam be classified as predicted nonspam since the user will just have to delete the email. On the other hand if actual nonspam is classified as spam, important messages may be lost and the user will have to constantly dig through their spam folder. For this reason, sensitivity and specificity are valuable measures. With this scenario we want low false negatives. Since higher values of false negatives decrease the sensitivity measure, we want to have high sensitivity. Therefore since the sesitivity values for additive and over are .942 and .7898 we should choose the additive method. In another less logical scenario where we cared more about the case where actual spam is classified as nonspam (False positive) we might consider taking the model over since it has a much higher speceficity (.9273 > .8892)

# Excercize 2

```{r}
bank = read.csv("bank.csv")
table(bank$y)
bank_idx = sample(nrow(bank), 4000)
bank_trn = bank[bank_idx, ]
bank_tst = bank[-bank_idx, ]
fit = glm(y ~ age + balance + campaign + previous +loan +duration + housing, data = bank_trn, family = binomial)
# Run cross fold validation
set.seed(1)
cv.glm(bank_trn, fit, K = 10)$delta[1]

bank_pred = ifelse(predict(fit, bank_tst) > 0,
                       "yes",
                       "no")

# Create confusion matrices for each
fit_matrix = make_conf_mat(predicted = bank_pred, actual = bank_tst$y)
mean(bank_pred != bank_tst$y)
sensitivity(fit_matrix)
specificity(fit_matrix)
summary(fit)
```

We created a model using age,balance,campaign,previous,loan,duration,housing. It does out perform classifying all observations as the majority case, but it is a somewhat weak model with low sensitivity. Since it has fairly high specificity it could be useful for a marketing campaign since clients predicted postive are very likely to sign with the bank, but it still lets quite a few yes's through the cracks. It would probably be more useful to bias the models predictions upwards and focus on the most likely candidates(even though the model predicts them as no).

The coeefecients on the model are:

Intercept -2.911e+00 This drives the model down, basically it is saying if everything else is 0 we should say this client will not subscribe.

age          5.079e-03  The coeeficent on age is .00078 as people grow older they are slightly more likely to subscribe. This is however not a statistically significant prediction.  

balance      2.473e-05  As balance increased people are slightly more likely to subscribe.

campaign    -9.836e-01  The number of contacts has a fairly strong negative relationship with likelihood to subscribe.

previous      3.757e-01    On the other hand the number of contacts performed before this campaign has a fairly strong positive effect.

duration     8.879e-03  This is the strongest predictor in our data, as duration increases it becomes more and more likely the result will be yes.

housingyes  -7.976e-01  If the person has a housing loan they are significantly less likely to subscribe.

``` {r}
fit_matrix
bank_pred = ifelse(predict(fit, bank_tst) > -2,
                       "yes",
                       "no")

# Create confusion matrices for each
fit_matrix = make_conf_mat(predicted = bank_pred, actual = bank_tst$y)
mean(bank_pred != bank_tst$y)
sensitivity(fit_matrix)
specificity(fit_matrix)
summary(fit)
fit_matrix
```
By biasing up like this I sacrifice specificity for sensitivity, but since a bank is mostly only concerned with people who say yes. I think this is a more valuable model, despite its worse absolute performance. 