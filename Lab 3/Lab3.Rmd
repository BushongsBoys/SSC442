---
title: "Lab3"
author: "Group 17"
date: "2/11/2020"
output: pdf_document
  #html_document:
  #  df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(plyr)
library(tidyverse)
library(ggplot2)
library(gbm)          # basic implementation
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}


# Function that assigns a number ot model complexity -> based on number of predictors in the model
get_complexity = function(model) {
  length(coef(model)) - 1
}

# Load Ames data from github
ames <- read.table("https://raw.githubusercontent.com/BushongsBoys/SSC442/master/Lab%203/ames1.csv",
                   header = TRUE,
                   sep = ",")

# Remove columns and one factor variables
ames <- ames[, -(which(names(ames) %in% c("OverallCond", "OverallQual")))]

# Fills NA values with previous value in column --> allows us to run a fullModel
ames_t <- as_tibble(ames)
ames_t <- ames_t %>% fill(names(.))

```

## Exercise 1 

The code below is 15 models we created based on a forward selection process. Criteria for adding the next parameter was which additional varaible results a lower RSS determined by the function step(). In addition we created a plot comparing the complexity of models to its Root Mean Squared Error. Complexity was measure by the number of predictors in a model. It is important to note that for categorical data, each dummy variable acts as a predictor. Threfore, a variable like Neighborhood with 25 levels would result in a complexity of 24. 

0.  nullModel <- lm(SalePrice ~ 1, data = ames_t)
1.  lm(SalePrice ~ Neighborhood, data = ames_t)
2.  lm(SalePrice ~ Neighborhood + GrLivArea, data = ames_t)
3.  lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual, data = ames_t)
4.  lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual + BsmtExposure, data = ames_t)
5.  lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl, data = ames_t)
6.  lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, 
       data = ames_t)
7.  lm(SalePrice ~ BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF,          data = ames_t)
8.  lm(SalePrice ~ Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl +         TotalBsmtSF, data = ames_t)
9.  lm(SalePrice ~ BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure +        RoofMatl + TotalBsmtSF, data = ames_t)
10. lm(SalePrice ~ BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual +           BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t)
11. lm(SalePrice ~ YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea +             KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t)
12. lm(SalePrice ~ ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood +             GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t)
13. lm(SalePrice ~ Functional + ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual +               Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t)
14. lm(SalePrice ~ LotArea + Functional + ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 +                BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF,
        data = ames_t)
15. lm(SalePrice ~ YearRemodAdd + LotArea + Functional + ExterQual + YearBuilt + BldgType + BsmtFinSF1 +             Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF,
       data = ames_t)


```{r, echo = FALSE }
# Forward selection process 
nullModel <- lm(SalePrice ~ 1, data = ames_t)
fullModel <- lm(SalePrice ~ ., data = ames_t)

# Detects the variable to add to the null model that results in the lowest RSS 
#step(nullModel, direction = "forward", scope = formula(fullModel))

# Models based on step fucntion up to complexity 15 in a list format 
models <- list(model1 <- lm(SalePrice ~ Neighborhood, data = ames_t),
model2 <- lm(SalePrice ~ Neighborhood + GrLivArea, data = ames_t),
model3 <- lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual, data = ames_t),
model4 <- lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual + BsmtExposure, data = ames_t),
model5 <- lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl, data = ames_t),
model6 <- lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t),
model7 <- lm(SalePrice ~ BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t),
model8 <- lm(SalePrice ~ Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t),
model9 <- lm(SalePrice ~ BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t),
model10 <- lm(SalePrice ~ BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t),
model11 <- lm(SalePrice ~ YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t),
model12 <- lm(SalePrice ~ ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t),
model13 <- lm(SalePrice ~ Functional + ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t),
model14 <- lm(SalePrice ~ LotArea + Functional + ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t),
model15 <- lm(SalePrice ~ YearRemodAdd + LotArea + Functional + ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = ames_t))

# Creates a dataframe containg the complexity and RMSE of each model
modelCompData <- data.frame(
  "Complexity" = complexities <- sapply(models, get_complexity),
  "RMSE" = modrmse <- sapply(models, function(x){rmse(ames_t$SalePrice, predict(x, ames_t))}))

# Create plot comparing RMSE to Complexity
modelCompPlot <- ggplot(data = modelCompData, mapping = aes(x= Complexity, y = RMSE)) + 
  geom_line() + 
  labs(title = "RMSE vs Model Complexity") + 
  theme(plot.title = element_text(hjust = .5, size = 18)) 

plot(modelCompPlot)
```


As we can see from the graph above, as the model gets more complex, the Root Mean Squared Error will decrease. However, just because the the mean squared error decreases does not mean we should use the full model. The RMSE is the measure by taking sqrt(mean(actual - predicted)^2), therfore, when we add more predictors to our model, it is going to fit it better and of course redcuce the RMSE. However, when making models, we are not looking to fit our sample with the best model possible, but find the real relationship of something. The full model will often overfit the data causing greater RMSE in the actual population. 
##Exercise 2
```{r, echo = FALSE }

set.seed(96)
num_obs = nrow(ames_t)

train_index = sample(num_obs, size = trunc(.9 * num_obs))
train_data = ames_t[train_index, ]
test_data = ames_t[-train_index, ]

get_rmse = function(model, data, response) {
  rmse(actual = subset(data, select = response, drop = TRUE),
       predicted = predict(model, data))
}
#models2
models2 <- list(model1 <- lm(SalePrice ~ Neighborhood, data = train_data),
               model2 <- lm(SalePrice ~ Neighborhood + GrLivArea, data = train_data),
               model3 <- lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual, data = train_data),
               model4 <- lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual + BsmtExposure, data = train_data),
               model5 <- lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl, data = train_data),
               model6 <- lm(SalePrice ~ Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data),
               model7 <- lm(SalePrice ~ BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data),
               model8 <- lm(SalePrice ~ Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data),
               model9 <- lm(SalePrice ~ BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data),
               model10 <- lm(SalePrice ~ BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data),
               model11 <- lm(SalePrice ~ YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data),
               model12 <- lm(SalePrice ~ ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data),
               model13 <- lm(SalePrice ~ Functional + ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data),
               model14 <- lm(SalePrice ~ LotArea + Functional + ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data),
               model15 <- lm(SalePrice ~ YearRemodAdd + LotArea + Functional + ExterQual + YearBuilt + BldgType + BsmtFinSF1 + Condition2 + BsmtQual + Neighborhood + GrLivArea + KitchenQual + BsmtExposure + RoofMatl + TotalBsmtSF, data = train_data))

train_rmse = sapply(models2, get_rmse, data = train_data, response = "SalePrice")
test_rmse = sapply(models2, get_rmse, data = test_data, response = "SalePrice")
model_complexity = sapply(models2, get_complexity)
plot(model_complexity, train_rmse, type = "b",
     ylim = c(min(c(train_rmse, test_rmse)) - 0.02,
              max(c(train_rmse, test_rmse)) + 0.02),
     col = "dodgerblue",
     xlab = "Model Size",
     ylab = "RMSE")
lines(model_complexity, test_rmse, type = "b", col = "darkorange")
min(test_rmse)


#excercize 2 
ames2 <- ames_t
ames2 <- fastDummies::dummy_cols(ames2, select_columns = c("Neighborhood","RoofMatl"))
nullModel <- lm(SalePrice ~ 1, data = ames2)
fullModel <- lm(SalePrice ~ ., data = ames2)

#step(nullModel, direction = "forward", scope = formula(fullModel))
set.seed(9)

num_obs = nrow(ames2)

train_index = sample(num_obs, size = trunc(.5 * num_obs))
train_data = ames2[train_index, ]
test_data = ames2[-train_index, ]
#right now best is 30539.99
best_model <- lm(formula = SalePrice ~ GrLivArea+ GrLivArea*GrLivArea + ExterQual + BsmtQual + GarageCars + 
                   BsmtFinSF1 + KitchenQual + MSSubClass + BsmtExposure + YearBuilt + 
                   Fireplaces + Functional + Condition1 + LotShape + LandContour + 
                   KitchenAbvGr + YearRemodAdd + MasVnrArea + MSZoning + LotFrontage + BedroomAbvGr, 
   data = train_data)
get_rmse(best_model, train_data, 'SalePrice')
get_rmse(best_model, test_data, 'SalePrice')
summary(best_model)

best_model <- gbm::gbm(SalePrice ~ GrLivArea+ GrLivArea*GrLivArea + ExterQual + BsmtQual + GarageCars + 
                         BsmtFinSF1 + KitchenQual + MSSubClass + BsmtExposure + YearBuilt + 
                         Fireplaces + Functional + Condition1 + LotShape + LandContour + 
                         KitchenAbvGr + YearRemodAdd + MasVnrArea + MSZoning + LotFrontage + BedroomAbvGr, 
                       data = train_data, 
                       # data set
                       verbose = FALSE, 
                       n.trees = 5000, 
                       cv.folds = 10
)
summary(best_model)
get_rmse(best_model, train_data, 'SalePrice')
get_rmse(best_model, test_data, 'SalePrice')
summary(best_model)
```
The best mean squared error we were able to achieve was 30539.99. To get this we used the best features from the data as found by the feed forward model. We removed roof type and neighborhood as they seemed to cause overfitting. Using GrLivArea squared increased the performance of the model probably due to an obeservable dimishing return to square footage in homes. Using a gradiant boosted tree regression through the gbm package also allowed us to perform slightly better than the linear regression. The test and train rmse are close which we took to be a good sign that we hadn't over or underfitted the data too much. We think our model overall will perform somewhat in the middle of the pack in the class. 